# Data-Science-Nailgun

An advanced package for streamlined data science experimentation with support for multiple data configurations, hyperparameter tuning, and automated model evaluation.

## Features

- **Multiple Data Configurations**: Support for different feature sets and preprocessing strategies
- **Hyperparameter Tuning**: Automated GridSearchCV for optimal model parameters
- **Experiment Organization**: Timestamped experiment directories with comprehensive result storage
- **Post-Experiment Analysis**: Tools for model evaluation and prediction generation
- **YAML Configuration**: Human-readable configuration files for experiments

## Installation

### Development Installation

```bash
# Clone the repository
git clone <repository-url>
cd data-science-nailgun

# Create virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install in development mode
pip install -e .
```

### Pre-commit Setup (Optional)

```bash
pip install pre-commit
pre-commit install
```

## Quick Start with Titanic Example

### 1. Get the Titanic Dataset

The Titanic dataset is already included in the repository. If you need to download it fresh:

```bash
# Create data directory
mkdir -p ds_nailgun/data/titanic

# Download from Kaggle (requires Kaggle API key)
# Or manually download from: https://www.kaggle.com/c/titanic/data
# Place train.csv and test.csv in ds_nailgun/data/titanic/
```

The expected file structure is:
```
ds_nailgun/data/titanic/
├── train.csv
└── test.csv
```

### 2. Run the Example Experiment

The package includes a complete example experiment that trains Random Forest and XGBoost models on the Titanic dataset with different feature configurations.

```bash
# Run the example experiment
python -c "
from ds_nailgun.nailgun.controller import ExperimentController
controller = ExperimentController('ds_nailgun/configs/examples/titanic_experiment_config.yaml')
controller.run_experiment()
print(f'Experiment completed! Results saved to: {controller.output_dir}')
"
```

This will:
- Load 2 different data configurations (different feature sets)
- Train 2 model types (Random Forest + XGBoost) with hyperparameter tuning
- Create a timestamped experiment directory with all results
- Save trained models, hyperparameter results (CSV), and experiment summary

### 3. Analyze Results with Post-Experiment Tool

After running the experiment, use the `post-experiment` command to analyze results and generate predictions:

```bash
# Analyze the latest experiment (replace with your experiment directory)
post-experiment experiments/titanic_survival_prediction_20250923_103134

# Or run with automatic 'yes' to generate predictions
echo "y" | post-experiment experiments/titanic_survival_prediction_20250923_103134
```

The post-experiment tool will:
- Load all trained models from the experiment
- Evaluate models on validation data (if available) or use CV scores
- Display detailed information about the best performing model
- Prompt for confirmation before generating test predictions
- Create `test_predictions.csv` in the experiment directory

## Experiment Directory Structure

After running an experiment, you'll find a timestamped directory like:

```
experiments/titanic_survival_prediction_20250923_103134/
├── configs/                    # Original configuration files
│   ├── experiment_config.yaml
│   ├── data_config_1.yaml
│   └── random_forest_classifier_config.yaml
├── experiment_summary.yaml     # Human-readable experiment summary
├── logs/
│   └── titanic_experiment.log
├── models/                     # Trained pipeline models (.joblib)
├── results/                    # Hyperparameter tuning results (.csv)
└── test_predictions.csv        # Generated by post-experiment tool
```

## Configuration Files

### Data Configuration

Define your dataset structure and feature groups:

```yaml
# Example: ds_nailgun/configs/examples/titanic_data_config.yaml
files:
  train_data: ds_nailgun/data/titanic/train.csv
  test_data: ds_nailgun/data/titanic/test.csv

data:
  id:
    column: "PassengerId"
  target:
    column: "Survived"
  features:
    passenger_info: ["Pclass", "SibSp", "Parch"]
    continuous_features: ["Age", "Fare"]
    categorical_features: ["Sex", "Embarked"]
```

### Experiment Configuration

Define the complete experiment setup:

```yaml
# Example: ds_nailgun/configs/examples/titanic_experiment_config.yaml
data:
  config_paths:
    - "ds_nailgun/configs/examples/titanic_data_config.yaml"

models:
  config_paths:
    - "ds_nailgun/configs/model_presets/random_forest_classifier_config.yaml"

experiment:
  name: "My Experiment"
  description: "Description of my experiment"
  random_seed: 42
```

## Command Line Tools

### Post-Experiment Analysis

```bash
# Show help
post-experiment --help

# Analyze experiment with user prompts
post-experiment /path/to/experiment/directory

# Skip confirmation prompts (for automation)
post-experiment --force /path/to/experiment/directory
```

## Advanced Usage

### Custom Data Configurations

Create multiple data configurations to test different feature engineering approaches:

1. Copy an existing data config
2. Modify the `features` section with different feature groups
3. Add the new config path to your experiment configuration

### Custom Model Configurations

Define your own models with hyperparameter grids:

```yaml
# Custom model config
model:
  type: "RandomForestClassifier"
  parameters:
    n_estimators: 100
    max_depth: 10

hypertuning:
  - param: "n_estimators"
    values: [50, 100, 200]
  - param: "max_depth"
    values: [5, 10, 15, null]
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and pre-commit checks
5. Submit a pull request

## License

[Add your license information here]
