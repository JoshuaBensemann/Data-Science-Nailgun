# GradientBoosting Quantile Regression Model Configuration (q=0.5)

model:
  type: GradientBoostingRegressor
  library: sklearn.ensemble
  parameters:
    n_estimators: 500  # Reduced from 1000 for faster training
    max_depth: 6
    learning_rate: 0.1
    min_samples_split: 2
    min_samples_leaf: 1
    subsample: 0.8  # Added subsample for better performance
    verbose: 0  # Silent
    # Removed early stopping parameters for full training
    # Quantile regression specific parameters
    loss: 'quantile'
    alpha: 0.5  # Quantile level (0.5 = 50th percentile)

hypertuning:
  method: "halving_random_search"  # Successive halving with random search for efficient large-scale optimization
  verbose: 2  # Verbose search progress
  n_candidates: 50  # Extensive number of candidates for thorough search
  parameters:
    max_depth: [3, 4, 5, 6, 8, 10]  # Extended depth range
    learning_rate: [0.005, 0.01, 0.05, 0.1, 0.2, 0.3]  # Broader learning rate range
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]  # Extended subsample range
    min_samples_split: [2, 5, 10, 20, 50]  # Extended minimum samples to split
    min_samples_leaf: [1, 2, 5, 10, 20]  # Extended minimum samples per leaf
    max_features: ['sqrt', 'log2', None, 0.5, 0.7, 0.9]  # Extended feature selection
    alpha: [0.1, 0.5, 0.9]  # Quantile levels for tuning
    verbose: [0]  # Silent models during tuning
  cv: 3  # Cross-validation folds
  scoring:
    name: 'pinball_loss'
    alpha: 0.5  # Quantile level for pinball loss scoring
  n_jobs: 1  # Halving search doesn't parallelize well
  factor: 3  # Halving factor - eliminate 2/3 of candidates each round
  resource: 'n_samples'  # Resource parameter to optimize (number of training samples)