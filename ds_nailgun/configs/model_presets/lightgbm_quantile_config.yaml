# LightGBM Quantile Regression Model Configuration (q=0.9)

model:
  type: LGBMRegressor
  library: lightgbm
  parameters:
    n_estimators: 1000  # Increased for full training without early stopping
    max_depth: 10  # Limited depth to prevent overfitting and split warnings
    num_leaves: 100  # Maximum number of leaves per tree
    learning_rate: 0.1
    subsample: 0.8
    feature_fraction: 0.8  # LightGBM's version of colsample_bytree
    min_child_samples: 10  # Minimum samples per leaf to reduce overfitting
    random_state: 1986
    lambda_l1: 0.0  # L1 regularization
    lambda_l2: 1.0  # L2 regularization
    min_split_gain: 0.001  # Small minimum gain threshold to avoid -inf warnings
    n_jobs: -1  # Use all available CPU cores
    verbosity: -1  # Completely silent models during tuning to suppress warnings
    # Quantile regression specific parameters
    objective: 'quantile'
    alpha: 0.9  # Quantile level (0.9 = 90th percentile)
    eval_metric: 'quantile'

hypertuning:
  method: "optuna"
  n_trials: 25  # Number of trials for Optuna
  direction: "minimize"  # For pinball loss, lower is better
  parameters:
    max_depth: {"type": "categorical", "choices": [6, 10, -1]}  # -1 means no limit
    learning_rate: {"type": "float", "low": 0.01, "high": 0.3, "log": true}  # Log scale for learning rate
    num_leaves: {"type": "int", "low": 31, "high": 255}  # From default LightGBM settings
    subsample: {"type": "float", "low": 0.5, "high": 1.0}  # Subsample ratio
    feature_fraction: {"type": "float", "low": 0.5, "high": 1.0}  # Feature fraction
    min_child_samples: {"type": "int", "low": 5, "high": 50}  # Min samples per leaf
    lambda_l1: {"type": "float", "low": 0.0, "high": 1.0}  # L1 regularization
    lambda_l2: {"type": "float", "low": 0.0, "high": 1.0}  # L2 regularization
    # verbose parameter removed - not needed for tuning
  cv: 3
  scoring:
    name: 'pinball_loss'
    alpha: 0.9
  n_jobs: 1
  factor: 3
  resource: 'n_samples'