# LightGBM Quantile Regression Model Configuration (q=0.9)

model:
  type: LGBMRegressor
  library: lightgbm
  parameters:
    n_estimators: 1000  # Increased for early stopping
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    random_state: 1986
    verbosity: 0  # Reduced verbosity for speed
    n_jobs: -1  # Use all available CPU cores
    # Quantile regression specific parameters
    objective: 'quantile'
    alpha: 0.9  # Quantile level (0.9 = 90th percentile)
    early_stopping_round: 50  # Enable early stopping for quantile regression

# Optional hyperparameter tuning configuration
hypertuning:
  method: "random_search"  # Options: "grid_search", "random_search"
  n_iter: 20  # Number of parameter combinations to try
  parameters:
    n_estimators: [100, 200, 500, 1000]
    max_depth: [3, 4, 5, 6, 8, 10, -1]  # -1 means no limit
    learning_rate: [0.01, 0.05, 0.1, 0.2, 0.3]
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
    colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]
    num_leaves: [20, 31, 50, 100]  # Added num_leaves for better control
    min_child_samples: [10, 20, 30, 50]  # Added min_child_samples
    verbosity: [-1]  # Silent during tuning
  cv: 3  # Reduced from 5 to 3 for speed
  scoring:
    name: 'pinball_loss'
    alpha: 0.9  # Quantile level for pinball loss scoring
  n_jobs: 1  # Use single core to avoid joblib resource leaks