# LightGBM Quantile Regression Model Configuration (q=0.9)

model:
  type: LGBMRegressor
  library: lightgbm
  parameters:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    random_state: 1986
    verbosity: 1  # Increased verbosity for more detailed output
    # Quantile regression specific parameters
    objective: 'quantile'
    alpha: 0.9  # Quantile level (0.9 = 90th percentile)

# Optional hyperparameter tuning configuration
hypertuning:
  method: "grid_search"  # Options: "grid_search", "random_search"
  parameters:
    n_estimators: [50, 100, 200]
    max_depth: [3, 6, 9]
    learning_rate: [0.01, 0.1, 0.2]
    subsample: [0.8, 1.0]
    colsample_bytree: [0.8, 1.0]
    verbosity: [1]  # Keep verbosity at info level during tuning
  cv: 5
  scoring:
    name: 'pinball_loss'
    alpha: 0.9  # Quantile level for pinball loss scoring