# LightGBM Quantile Regression Model Configuration (q=0.5)

model:
  type: LGBMRegressor
  library: lightgbm
  parameters:
    n_estimators: 1000  # Increased for full training without early stopping
    max_depth: 10  # Limited depth to prevent overfitting and split warnings
    num_leaves: 100  # Maximum number of leaves per tree
    learning_rate: 0.1
    subsample: 0.8
    feature_fraction: 0.8  # LightGBM's version of colsample_bytree
    min_child_samples: 10  # Minimum samples per leaf to reduce overfitting
    random_state: 1986
    lambda_l1: 0.0  # L1 regularization
    lambda_l2: 1.0  # L2 regularization
    min_split_gain: 0.001  # Small minimum gain threshold to avoid -inf warnings
    n_jobs: -1  # Use all available CPU cores
    verbosity: -1  # Completely silent models during tuning to suppress warnings
    # Quantile regression specific parameters
    objective: 'quantile'
    alpha: 0.5  # Quantile level (0.5 = 50th percentile)
    eval_metric: 'quantile'

hypertuning:
  method: "halving_random_search"  # Successive halving with random search for efficient large-scale optimization
  verbose: 2  # Verbose search progress
  n_candidates: 50  # Extensive number of candidates for thorough search
  parameters:
    max_depth: [6, 8, 10, 12, 15, -1]  # Extended depth range including unlimited
    learning_rate: [0.005, 0.01, 0.05, 0.1, 0.2, 0.3]  # Broader learning rate range
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]  # Extended subsample range
    feature_fraction: [0.6, 0.7, 0.8, 0.9, 1.0]  # Extended feature fraction range
    num_leaves: [20, 50, 100, 150, 200, 250]  # Extended leaf count range
    min_child_samples: [1, 5, 10, 20, 30, 50]  # Reduced minimum child samples to allow more splits
    min_split_gain: [0.0, 0.0001, 0.001, 0.01]  # Much lower minimum split gain to prevent -inf warnings
    lambda_l1: [0.0, 0.001, 0.01, 0.1, 1.0]  # L1 regularization
    lambda_l2: [0.0, 0.001, 0.01, 0.1, 1.0]  # L2 regularization
    verbose: [-1]  # Completely silent models during tuning to suppress warnings
  cv: 3  # Cross-validation folds
  scoring:
    name: 'pinball_loss'
    alpha: 0.5  # Quantile level for pinball loss scoring
  n_jobs: 1  # Halving search doesn't parallelize well
  factor: 3  # Halving factor - eliminate 2/3 of candidates each round
  resource: 'n_samples'  # Resource parameter to optimize (number of training samples)