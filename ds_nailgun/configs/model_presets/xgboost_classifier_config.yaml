# XGBoost Classifier Model Configuration

model:
  type: XGBClassifier
  library: xgboost
  parameters:
    n_estimators: 500  # Reduced from 1000 for faster training
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    random_state: 1986
    verbosity: 2  # Verbose mode
    n_jobs: -1  # Use all available CPU cores

# Optional hyperparameter tuning configuration
hypertuning:
  method: "halving_random_search"  # More efficient than grid search
  n_candidates: 50  # Extensive number of candidates for thorough search
  parameters:
    max_depth: [3, 4, 6, 8, 10, 12]  # Extended depth range
    learning_rate: [0.005, 0.01, 0.05, 0.1, 0.2, 0.3]  # Broader learning rate range
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]  # Extended subsample range
    colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]  # Extended feature fraction range
    gamma: [0, 0.1, 0.2, 0.5, 1.0]  # Extended minimum loss reduction
    min_child_weight: [1, 3, 5, 10, 20]  # Extended minimum child weight
    reg_alpha: [0, 0.001, 0.01, 0.1, 1.0]  # Extended L1 regularization
    reg_lambda: [0.1, 0.5, 1.0, 1.5, 2.0]  # Extended L2 regularization
  cv: 3  # Cross-validation folds
  scoring:
    name: 'accuracy'
  n_jobs: 1  # Halving search doesn't parallelize well
  factor: 3  # Halving factor - eliminate 2/3 of candidates each round
  resource: 'n_samples'  # Resource parameter to optimize (number of training samples)]
  