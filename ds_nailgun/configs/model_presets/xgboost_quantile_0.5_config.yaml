# XGBoost Quantile Regression Model Configuration (q=0.5)

model:
  type: XGBRegressor
  library: xgboost
  parameters:
    n_estimators: 500  # Reduced from 1000 for faster training
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    random_state: 1986
    tree_method: 'hist'  # Use histogram-based algorithm for faster training
    reg_alpha: 0.0  # L1 regularization
    reg_lambda: 1.0  # L2 regularization
    verbosity: 0  # Silent mode
    n_jobs: -1  # Use all available CPU cores
    # Quantile regression specific parameters
    objective: 'reg:quantileerror'
    quantile_alpha: 0.5  # Quantile level (0.5 = 50th percentile)

hypertuning:
  method: "halving_random_search"
  n_candidates: 30  # Reduced from 50 for more manageable search
  parameters:
    max_depth: [3, 6, 9]  # Focused depth range
    learning_rate: [0.01, 0.1, 0.2]  # Key learning rates
    n_estimators: [100, 300]  # Matching estimator counts
    subsample: [0.8, 1.0]  # Reasonable subsample range
    colsample_bytree: [0.8, 1.0]  # Reasonable feature fraction
    reg_alpha: [0, 0.1]  # L1 regularization options
    reg_lambda: [1.0, 5.0]  # L2 regularization options
  cv: 3
  scoring:
    name: 'pinball_loss'
    alpha: 0.5
  n_jobs: 1
  factor: 3
  resource: 'n_samples'