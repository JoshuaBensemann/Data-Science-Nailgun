# Random Forest Classifier Model Configuration

model:
  type: RandomForestClassifier
  library: sklearn.ensemble
  parameters:
    n_estimators: 200  # Increased from 100 for better performance
    max_depth: null
    min_samples_split: 2
    min_samples_leaf: 1
    random_state: 1986
    verbose: 0  # Reduced verbosity for cleaner output
    n_jobs: -1  # Use all available CPU cores

hypertuning:
  method: "halving_random_search"  # More efficient than grid search
  verbose: 2  # Verbose search progress
  n_candidates: 50  # Extensive number of candidates for thorough search
  parameters:
    n_estimators: [50, 100, 200, 300, 500]  # Extended estimator range
    max_depth: [None, 10, 20, 30, 50]  # Extended depth range
    min_samples_split: [2, 5, 10, 15, 20]  # Extended minimum samples to split
    min_samples_leaf: [1, 2, 4, 8, 10]  # Extended minimum samples per leaf
    max_features: ['sqrt', 'log2', None, 0.5, 0.7, 0.9]  # Extended feature selection
    criterion: ['gini', 'entropy', 'log_loss']  # Extended split criteria
    bootstrap: [True, False]  # Bootstrap options
    max_samples: [0.6, 0.8, 1.0]  # Bootstrap sample fractions
    verbose: [0]  # Silent models during tuning
  cv: 3  # Cross-validation folds
  scoring:
    name: 'accuracy'
  n_jobs: 1  # Halving search doesn't parallelize well
  factor: 3  # Halving factor - eliminate 2/3 of candidates each round
  resource: 'n_samples'  # Resource parameter to optimize (number of training samples)
  