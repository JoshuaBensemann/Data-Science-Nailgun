# CatBoost Quantile Regression Model Configuration (q=0.9)

model:
  type: CatBoostRegressor
  library: catboost
  parameters:
    iterations: 1000  # Number of boosting iterations
    depth: 6  # Depth of the tree
    learning_rate: 0.1
    subsample: 0.8  # Sample rate for bagging
    colsample_bylevel: 0.8  # Feature fraction per level
    random_state: 1986
    task_type: 'CPU'  # Can be changed to 'GPU' if GPU is available
    devices: '0'  # GPU device IDs (only used if task_type is GPU)
    # Quantile regression specific parameters
    loss_function: 'Quantile:alpha=0.9'  # Quantile loss with 0.9 quantile level

# Optional hyperparameter tuning configuration
hypertuning:
  method: "halving_random_search"  # Successive halving with random search for efficient large-scale optimization
  n_candidates: 90  # Number of candidate parameter combinations to evaluate initially
  n_iter: 20  # Number of parameter combinations to try
  parameters:
    depth: [4, 5, 6, 7, 8, 9, 10]
    learning_rate: [0.01, 0.05, 0.1, 0.15, 0.2, 0.3]
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
    colsample_bylevel: [0.6, 0.7, 0.8, 0.9, 1.0]
    l2_leaf_reg: [1, 3, 5, 7, 9]  # L2 regularization
    min_data_in_leaf: [1, 3, 5, 10, 20]  # Minimum samples per leaf
    verbose: [0]  # Silent during tuning
  cv: 3  # Cross-validation folds
  scoring:
    name: 'pinball_loss'
    alpha: 0.9  # Quantile level for pinball loss scoring
  n_jobs: 1  # Halving search doesn't parallelize well
  factor: 3  # Halving factor - eliminate 2/3 of candidates each round
  resource: 'n_samples'  # Resource parameter to optimize (number of training samples)
  max_resources: 'auto'  # Use all available training samples
  min_resources: 50000  # Minimum samples to start with (about 10% of typical training sets)