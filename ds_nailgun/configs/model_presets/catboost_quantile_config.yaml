# CatBoost Quantile Regression Model Configuration (q=0.9)

model:
  type: CatBoostRegressor
  library: catboost
  parameters:
    iterations: 500  # Reduced from 1000 for faster training
    depth: 6  # Depth of the tree
    learning_rate: 0.1
    subsample: 0.8  # Sample rate for bagging
    colsample_bylevel: 0.8  # Feature fraction per level
    random_state: 1986
    verbose: 0  # Silent mode
    # Quantile regression specific parameters
    loss_function: 'Quantile:alpha=0.9'  # Quantile loss with 0.9 quantile level

hypertuning:
  method: "optuna"
  n_trials: 25  # Number of trials for Optuna
  direction: "minimize"  # For pinball loss, lower is better
  parameters:
    depth: {"type": "int", "low": 3, "high": 10}  # Depth from 3 to 10
    learning_rate: {"type": "float", "low": 0.01, "high": 0.3, "log": true}  # Log scale for learning rate
    subsample: {"type": "float", "low": 0.5, "high": 1.0}  # Subsample ratio
    colsample_bylevel: {"type": "float", "low": 0.5, "high": 1.0}  # Feature fraction
    l2_leaf_reg: {"type": "float", "low": 1.0, "high": 10.0}  # L2 regularization
    min_data_in_leaf: {"type": "int", "low": 1, "high": 20}  # Min samples in leaf
    border_count: {"type": "categorical", "choices": [32, 64, 128, 254]}  # Border count options
    # verbose parameter removed - not needed for tuning
  cv: 3
  scoring:
    name: 'pinball_loss'
    alpha: 0.9
  n_jobs: 1
  factor: 3
  resource: 'n_samples'
  