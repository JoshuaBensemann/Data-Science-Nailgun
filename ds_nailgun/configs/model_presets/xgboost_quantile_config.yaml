# XGBoost Quantile Regression Model Configuration (q=0.9)

model:
  type: XGBRegressor
  library: xgboost
  parameters:
    n_estimators: 500  # Reduced from 1000 for faster training
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    random_state: 1986
    tree_method: 'hist'  # Use histogram-based algorithm for faster training
    reg_alpha: 0.0  # L1 regularization
    reg_lambda: 1.0  # L2 regularization
    verbosity: 0  # Silent mode
    n_jobs: -1  # Use all available CPU cores
    # Quantile regression specific parameters
    objective: 'reg:quantileerror'
    quantile_alpha: 0.9  # Quantile level (0.9 = 90th percentile)

hypertuning:
  method: "optuna"
  n_trials: 25  # Number of trials for Optuna
  direction: "minimize"  # For pinball loss, lower is better
  parameters:
    max_depth: {"type": "int", "low": 3, "high": 10}  # Depth from 3 to 10
    learning_rate: {"type": "float", "low": 0.01, "high": 0.3, "log": true}  # Log scale for learning rate
    n_estimators: {"type": "int", "low": 50, "high": 500, "step": 50}  # From 50 to 500 in steps of 50
    subsample: {"type": "float", "low": 0.5, "high": 1.0}  # Subsample ratio
    colsample_bytree: {"type": "float", "low": 0.5, "high": 1.0}  # Feature fraction
    gamma: {"type": "float", "low": 0.0, "high": 0.5}  # Min split loss
    min_child_weight: {"type": "int", "low": 1, "high": 10}  # Min child weight
    reg_alpha: {"type": "float", "low": 0.0, "high": 0.5}  # L1 regularization
    # verbosity parameter removed - not needed for tuning
  cv: 3
  scoring:
    name: 'pinball_loss'
    alpha: 0.9
  n_jobs: 1
  factor: 3
  resource: 'n_samples'
