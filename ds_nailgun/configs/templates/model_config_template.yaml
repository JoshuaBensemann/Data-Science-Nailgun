# Model Configuration Template
# This template shows all available options for configuring a model in Data-Science-Nailgun
# Copy this file and customize it for your specific model

# Model configuration section - defines the model type and parameters
model:
  # Model class name (e.g., RandomForestClassifier, XGBClassifier, RandomForestRegressor, LogisticRegression, LinearRegression)
  type: "RandomForestClassifier"
  # Library/module where the model is located (e.g., sklearn.ensemble, xgboost, sklearn.linear_model)
  library: "sklearn.ensemble"
  # Model parameters - these are passed directly to the model constructor
  parameters:
    # Common parameters for RandomForest models
    n_estimators: 100  # Number of trees in the forest
    max_depth: null    # Maximum depth of the tree (null for unlimited)
    min_samples_split: 2  # Minimum samples required to split an internal node
    min_samples_leaf: 1   # Minimum samples required to be at a leaf node
    random_state: 42     # Random state for reproducibility
    # For XGBoost models, you might use:
    # learning_rate: 0.1
    # subsample: 0.8
    # colsample_bytree: 0.8
    # Add other model-specific parameters as needed
    # For example, for LogisticRegression:
    # C: 1.0
    # penalty: "l2"

# Optional hyperparameter tuning configuration - for automated parameter optimization
hypertuning:
  # Tuning method: "halving_random_search" for efficient successive halving with random search
  # Other options: "halving_grid_search", "random_search", "grid_search"
  method: "halving_random_search"
  # Number of candidate parameter combinations to evaluate initially (for random search methods)
  n_candidates: 80
  # Parameter space to search - lists for grid search, can also include distributions for random search
  parameters:
    # Example parameters for RandomForest tuning
    n_estimators: [50, 100, 200, 300, 500]
    max_depth: [null, 5, 10, 15, 20, 25, 30, 40]
    min_samples_split: [2, 3, 5, 7, 10, 15]
    min_samples_leaf: [1, 2, 3, 4, 5, 8, 10]
    max_features: ['sqrt', 'log2', None, 0.3, 0.5, 0.7, 0.9]
    criterion: ['gini', 'entropy', 'log_loss']  # For classification
    # criterion: ['squared_error', 'absolute_error', 'friedman_mse']  # For regression
    min_impurity_decrease: [0.0, 0.0001, 0.001, 0.01]
    bootstrap: [True, False]
    oob_score: [False]  # Only when bootstrap=True
    # For XGBoost tuning, you might use:
    # learning_rate: [0.01, 0.05, 0.1, 0.2, 0.3]
    # max_depth: [3, 4, 5, 6, 7, 8, 9, 10]
    # subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
    # colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]
    # min_child_weight: [1, 3, 5, 7]
    # gamma: [0.0, 0.1, 0.2, 0.3, 0.4]
    # reg_alpha: [0, 0.001, 0.01, 0.1, 1]
    # reg_lambda: [0.1, 0.5, 1, 1.5, 2]
  # Cross-validation folds
  cv: 3
  # Scoring metric - depends on problem type
  # For classification: "accuracy", "f1", "precision", "recall", "roc_auc", "neg_log_loss"
  # For regression: "neg_mean_squared_error", "neg_mean_absolute_error", "r2", "neg_median_absolute_error"
  scoring:
    name: "accuracy"  # For classification
    # For quantile regression with pinball loss:
    # name: "pinball_loss"
    # alpha: 0.9  # Quantile level (0.1, 0.5, 0.9, etc.)
  # Number of parallel jobs (set to 1 for halving search to avoid resource conflicts)
  n_jobs: 1
  # Halving search parameters (only used for halving methods)
  factor: 3  # Elimination factor - remove 2/3 of candidates each round
  resource: 'n_estimators'  # Resource parameter to optimize (usually n_estimators)
  max_resources: 1000  # Maximum resources to use in final round
  min_resources: 25  # Minimum resources to start with</content>  # Only used for random_search